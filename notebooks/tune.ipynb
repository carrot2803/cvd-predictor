{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/Documents/cvd-predictor/\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from CVD.utils import get_metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df: pl.DataFrame = pl.read_parquet(\"data/intermediate/heart_cdc_2023_cleaned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X: pl.DataFrame = df.drop([\"CVD\"])\n",
    "y: pl.Series = df[\"CVD\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X[X.columns] = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-8, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"scale_pos_weight\": trial.suggest_loguniform(\"scale_pos_weight\", 1e-2, 10),  # For handling imbalance\n",
    "        \"random_state\": 42,\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"use_label_encoder\": False,\n",
    "    }\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    model = XGBClassifier(**params)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=skf, scoring=\"f1\", n_jobs=-1)\n",
    "    return scores.mean()\n",
    "\n",
    "# Create Optuna study and optimize\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "print(f\"  F1 Score: {best_trial.value}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb = XGBClassifier(**best_trial.params, random_state=42, eval_metric=\"logloss\", use_label_encoder=False)\n",
    "best_xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_prob = best_xgb.predict_proba(X_test)[:, 1]\n",
    "y_pred_binary = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "test_f1 = f1_score(y_test, y_pred_binary, average=\"weighted\")\n",
    "print(f\"Test F1 Score: {test_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results: list[dict] = []\n",
    "results.append(get_metrics(y_test, y_pred_binary, \"XGBoost\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_xgb, open(\"models/xgb_cvd.pkl\", \"wb\"))\n",
    "xgb = pickle.load(open(\"models/xgb_cvd.pkl\", \"rb\"))\n",
    "y_pred_prob = xgb.predict_proba(X_test)[:, 1]\n",
    "pl.DataFrame(get_metrics(y_test, (y_pred_prob >= 0.5).astype(int), \"XGBoost\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "        # LightGBM uses bagging_fraction (equivalent to subsample) and bagging_freq to enable bagging\n",
    "        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
    "        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
    "        # min_split_gain is LightGBM's equivalent to gamma in XGBoost\n",
    "        \"min_split_gain\": trial.suggest_loguniform(\"min_split_gain\", 1e-8, 1.0),\n",
    "        # min_child_samples corresponds to the minimum number of samples in a leaf\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 1, 10),\n",
    "        \"scale_pos_weight\": trial.suggest_loguniform(\"scale_pos_weight\", 1e-2, 10),  # For handling imbalance\n",
    "        \"random_state\": 42,\n",
    "        \"eval_metric\": \"logloss\",\n",
    "    }\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    model = LGBMClassifier(**params)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=skf, scoring=\"f1\", n_jobs=-1)\n",
    "    return scores.mean()\n",
    "\n",
    "# Create an Optuna study and optimize the objective\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best trial details\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "print(f\"  F1 Score: {best_trial.value}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lgb = LGBMClassifier(**best_trial.params, random_state=42, eval_metric=\"logloss\")\n",
    "best_lgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_prob: np.ndarray = best_lgb.predict_proba(X_test)[:, 1]\n",
    "y_pred_binary:  np.ndarray = (y_pred_prob >= 0.5).astype(int)\n",
    "test_f1: float = f1_score(y_test, y_pred_binary, average=\"weighted\")\n",
    "print(f\"Test F1 Score: {test_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append(get_metrics(y_test, y_pred_binary, \"LightGBM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_lgb, open(\"models/lightgbm_cvd.pkl\", \"wb\"))\n",
    "lgb: LGBMClassifier = pickle.load(open(\"models/lightgbm_cvd.pkl\", \"rb\"))\n",
    "y_pred_prob: np.ndarray = lgb.predict_proba(X_test)[:, 1]\n",
    "y_pred_binary:  np.ndarray = (y_pred_prob >= 0.5).astype(int)\n",
    "pl.DataFrame(get_metrics(y_test, y_pred_binary, \"LightGBM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame(results)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
